{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "328eed10-2aa0-48f6-bd98-0aeb57e376d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Preparation Notebook\n",
    "\n",
    "This notebook handles document loading, preprocessing, and vector index creation for the Maverick RAG system.\n",
    "\n",
    "## Features\n",
    "- Document loading from local files or Unity Catalog volumes\n",
    "- Text chunking and preprocessing\n",
    "- FAISS index creation (local development)\n",
    "- Databricks vector search setup (enterprise deployment)\n",
    "- Delta Lake table creation\n",
    "- Configuration validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3649cdcb-f82d-4407-9d3d-0c5be9033427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%pip install -qU databricks-sdk databricks-langchain databricks-agents mlflow[databricks] databricks-vectorsearch langchain langchain_core bs4 markdownify pydantic sentence-transformers pandas openpyxl langdetect\n",
    "#dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7eedaaa-029f-4ad4-9b00-8510b40eb0ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2c0f305-2155-4b00-be2f-ad37d9405f8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pkg_resources\n",
    "\n",
    "python_executable = sys.executable\n",
    "print(f\"Python Executable: {python_executable}\")\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"Installed Libraries:\")\n",
    "\n",
    "try:\n",
    "    sdk_version = pkg_resources.get_distribution(\"databricks-sdk\").version\n",
    "    print(f\"  - databricks-sdk: {sdk_version}\")\n",
    "except pkg_resources.DistributionNotFound:\n",
    "    print(\"  - databricks-sdk: Not installed\")\n",
    "\n",
    "import databricks\n",
    "print(f\"üêç Databricks loaded from: {databricks.__file__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f1da97d-9ae5-4f4d-9147-64fd9fbbb0e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# --- CORRECTED IMPORTS ---\n",
    "# Imports are now absolute from the project root, which is best practice\n",
    "from src.utils.config import config_manager\n",
    "from src.rag.ingest import load_documents, build_index, build_databricks_artifacts\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a7e8803-f390-476e-9097-8a60b454ccba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "181744c2-601e-4757-a082-abe91b323a70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Current Configuration:\n",
      "\n",
      "APP:\n",
      "  server_name: 0.0.0.0\n",
      "  server_port: 7860\n",
      "  title: Maverick RAG\n",
      "\n",
      "LLM:\n",
      "  provider: ollama\n",
      "  model: llama3.1:8b\n",
      "  temperature: 0.2\n",
      "  timeout: 60\n",
      "  api_key: ********\n",
      "  base_url: http://localhost:11434/v1\n",
      "\n",
      "EMBEDDING:\n",
      "  model_name: sentence-transformers/all-MiniLM-L6-v2\n",
      "  provider: huggingface\n",
      "\n",
      "DATA:\n",
      "  docs_dir: D:\\Work\\E D C\\poc_p1\\data\\docs\\sample_data\n",
      "  index_dir: D:\\Work\\E D C\\poc_p1\\data\\index\n",
      "  use_databricks: False\n",
      "  volume_path: None\n",
      "\n",
      "DATABRICKS:\n",
      "  workspace_url: your_workspace_url.databricks.com\n",
      "  access_token: ********\n",
      "  catalog: main\n",
      "  schema: default\n",
      "  volume: source_data\n",
      "  vector_search_endpoint: your_vector_search_endpoint\n",
      "  model_serving_endpoint: your_model_serving_endpoint\n",
      "  _client: None\n",
      "\n",
      "üîç Configuration Validation: ‚úÖ Valid\n"
     ]
    }
   ],
   "source": [
    "# Display current configuration\n",
    "config_summary = config_manager.get_config_summary()\n",
    "print(\"üìã Current Configuration:\")\n",
    "for section, config in config_summary.items():\n",
    "    print(f\"\\n{section.upper()}:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Validate configuration\n",
    "validation = config_manager.validate_config()\n",
    "print(f\"\\nüîç Configuration Validation: {'‚úÖ Valid' if validation['valid'] else '‚ùå Invalid'}\")\n",
    "\n",
    "if validation['errors']:\n",
    "    print(\"\\n‚ùå Errors:\")\n",
    "    for error in validation['errors']:\n",
    "        print(f\"  - {error}\")\n",
    "\n",
    "if validation['warnings']:\n",
    "    print(\"\\n‚ö†Ô∏è Warnings:\")\n",
    "    for warning in validation['warnings']:\n",
    "        print(f\"  - {warning}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78012e51-baf2-4261-a927-60f181b3a5c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Document Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ab3a176-ccae-425d-ae30-0ecba0a6c24c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Loading documents from: D:\\Work\\E D C\\poc_p1\\data\\docs\\sample_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'D:\\\\Work\\\\E D C\\\\poc_p1\\\\data\\\\docs\\\\sample_data\\\\Project_Book_Databricks_Asset_Bundle.docx'}, page_content='Project Book ‚Äî for GenAI & ML\\n\\nProduction-ready patterns for ML and LLM/GenAI on Azure Databricks\\n\\nVersion: 1.1 | Date: 2025-09-10\\n\\n1. Executive Summary\\n\\nThis Project Book defines a production-grade template and runbook for delivering machine learning (ML) and generative AI (GenAI) applications using Databricks Asset Bundles (DAB). It separates concerns between ML (data science, modeling, evaluation) and MLOps (platform, deployment, governance, observability), and codifies the repository structure, bundle configuration (databricks.yml), CI/CD, security, and operations.\\n\\n2. Scope & Audience\\n\\nAudience: ML Engineers, Data Scientists, Platform/MLOps Engineers, Security & Governance, Product Owners.\\n\\nScope: Applies to both traditional ML (forecasting, classification, etc.) and GenAI (RAG, agents, copilots).\\n\\nCloud/Platform: Azure Databricks with Unity Catalog, MLflow, Mosaic AI, Entra ID, Serverless Compute.\\n\\n3. Architecture Overview\\n\\n3.1 Four-layer Reference (from Project Charter)\\n\\nData Foundation (GraphRAG): Delta Lake + Knowledge Graph (Spark GraphX/GraphFrames) for long-term memory.\\n\\nTooling & Integration (Model Context Protocol, MCP): standardized, secure tool connectors.\\n\\nIntelligence & Reasoning (Multi-Agent System): OpenAI Assistants/Agents orchestration with LangChain.\\n\\nFoundational LLM (Hosted in North America): choice of Llama, GPT, Claude; governed access via Mosaic AI.\\n\\n3.2 Confirmed Stack (from Project Charter)\\n\\nCloud: Microsoft Azure; Platform: Databricks (Serverless where possible).\\n\\nData & AI Platform: Mosaic AI; Governance: Unity Catalog; Tracking: MLflow.\\n\\nAuthN/Z: Microsoft Entra ID (SSO, OAuth/M2M).\\n\\nAgent Framework: OpenAI Assistants, LangChain; UI: Gradio (or app frameworks).\\n\\n\\n\\n4. ML vs MLOps ‚Äî Separation of Concerns\\n\\n4.1 ML (Product/Use-case scope)\\n\\nDefine business problem & KPIs; curate datasets; feature engineering; model training & evaluation.\\n\\nPrompt/agent design, RAG pipelines, tool selection (MCP), offline & online evaluation.\\n\\nRegister models & app artifacts in MLflow / Unity Catalog; author notebooks & src modules; tests.\\n\\n4.2 MLOps (Platform/Operations scope)\\n\\nBundle configuration (databricks.yml), environments (dev/stage/prod) and deployment modes.\\n\\nProvision & manage compute (serverless where possible), permissions, and secret management.\\n\\nObservability (MLflow Tracing), model/app serving, cost/usage tracking via Mosaic AI Gateway.\\n\\nCI/CD automation, policy checks, data/AI governance via Unity Catalog.\\n\\n5. Databricks Asset Bundles (DAB) Overview\\n\\n5.1 Why Bundles\\n\\nDeclarative, versioned deployments for jobs, pipelines, models, serving endpoints, and related resources.\\n\\nEnvironment-specific targets (dev/stage/prod) with validation and deployment modes.\\n\\nRepeatable automation in CI/CD with bundle validate/deploy/run commands.\\n\\n5.2 Key Files & Concepts\\n\\ndatabricks.yml at repo root defines bundle name, targets, artifacts, variables, workspace settings.\\n\\nresources section defines jobs, pipelines, models, connections, volumes, dashboards, apps.\\n\\ntargets allow environment-specific overrides (compute, permissions, schedules, variables).\\n\\n6. Repository & Project Folder Structure\\n\\nUse the following structure for any ML or GenAI project. The /bundle folder contains DAB configuration; ML code is modularized under /src; notebooks are for exploration; pipelines and tests are first-class citizens.\\n\\n/ (repo root)\\n/ ‚îú‚îÄ‚îÄ app/ ‚îÇ   ‚îú‚îÄ‚îÄ app.py ‚îÇ   ‚îú‚îÄ‚îÄ settings.py ‚îÇ   ‚îî‚îÄ‚îÄ prompts/ ‚îÇ       ‚îú‚îÄ‚îÄ system/ ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ agent_system.md ‚îÇ       ‚îî‚îÄ‚îÄ rag/ ‚îÇ           ‚îî‚îÄ‚îÄ retrieval_prompt.md ‚îú‚îÄ‚îÄ src/ ‚îÇ   ‚îú‚îÄ‚îÄ config/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ settings.py ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ secrets_provider.py ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ registry.py ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py ‚îÇ   ‚îú‚îÄ‚îÄ logging/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logger.py ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mlflow_logging.py ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tracing.py ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py ‚îÇ   ‚îú‚îÄ‚îÄ data/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ loaders/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ processing/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schemas/ ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ graph/ ‚îÇ   ‚îú‚îÄ‚îÄ evaluation/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluators/ ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ reports/ ‚îÇ   ‚îú‚îÄ‚îÄ ai/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ training/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inference/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embeddings/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ retrieval/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompts/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agents/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ generation/ ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ registry/ ‚îÇ   ‚îú‚îÄ‚îÄ pipelines/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ training_pipeline.py ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validation_pipeline.py ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ testing_pipeline.py ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ inference_pipeline.py ‚îÇ   ‚îú‚îÄ‚îÄ serving/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ endpoints.py ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validators.py ‚îÇ   ‚îî‚îÄ‚îÄ utils/ ‚îÇ       ‚îú‚îÄ‚îÄ io.py ‚îÇ       ‚îú‚îÄ‚îÄ decorators.py ‚îÇ       ‚îî‚îÄ‚îÄ constants.py ‚îú‚îÄ‚îÄ bundle/ ‚îÇ   ‚îú‚îÄ‚îÄ databricks.yml ‚îÇ   ‚îî‚îÄ‚îÄ resources/ ‚îÇ       ‚îú‚îÄ‚îÄ jobs.yml ‚îÇ       ‚îú‚îÄ‚îÄ pipelines.yml ‚îÇ       ‚îú‚îÄ‚îÄ connections.yml ‚îÇ       ‚îú‚îÄ‚îÄ serving_endpoints.yml ‚îÇ       ‚îî‚îÄ‚îÄ permissions.yml ‚îú‚îÄ‚îÄ docs/ ‚îÇ   ‚îú‚îÄ‚îÄ index.md ‚îÇ   ‚îî‚îÄ‚îÄ architecture.md ‚îú‚îÄ‚îÄ .github/ ‚îÇ   ‚îî‚îÄ‚îÄ workflows/ ‚îÇ       ‚îî‚îÄ‚îÄ databricks_deploy.yml ‚îú‚îÄ‚îÄ requirements.txt ‚îú‚îÄ‚îÄ README.md ‚îî‚îÄ‚îÄ LICENSE\\n\\n7. Governance & Security\\n\\nUnity Catalog: single source of truth for data, models, volumes; assign privileges to groups; avoid user-level grants.\\n\\nUse service principals for automation; manage identities in Entra ID; prefer OAuth tokens over PATs.\\n\\nStore secrets in Unity Catalog connections or secret scopes; never commit secrets.\\n\\nAdopt serverless compute for least-privilege data plane and simpler ops; lock down production endpoints.\\n\\n8. Observability & Monitoring\\n\\nMLflow: experiment tracking, model registry (aliases like @champion/@challenger), lineage to runs.\\n\\nMosaic AI Gateway: usage tracking, payload logging (Delta tables), guardrails, rate limits, fallbacks.\\n\\n\\n\\nMLflow Tracing: capture prompts, tool calls, latencies; wire into dashboards and alerts.\\n\\nQuality monitors: schedule evaluation jobs; track costs/latency & business KPIs.\\n\\n9. Testing & Quality Gates\\n\\nUnit tests for src modules; integration tests for end-to-end pipelines; security tests for auth and inputs.\\n\\nBundle validation in CI; policy checks (lint YAML, prohibited settings).\\n\\nOffline eval datasets; automated acceptance thresholds (retrieval precision/recall, hallucination rate).\\n\\nBlue/green model rollout using model registry aliases; automatic rollback on SLO breaches.\\n\\n10. Runbooks & Operational Procedures\\n\\nCommon Databricks Asset Bundle commands:\\n\\ndatabricks bundle validate -t <target>\\ndatabricks bundle deploy -t <target>\\ndatabricks bundle run <resource-name> -t <target>\\ndatabricks bundle destroy -t <target>\\n\\n11. Appendices\\n\\nA. Sample RAG application structure (adapted)\\n\\nSee notebooks and src layout above; align with your current RAG structure to ensure parity with model tracking and retrieval components.\\n\\nB. MCP Tooling Layer\\n\\nExpose enterprise tools via MCP servers; register tools in agents/tools_mcp.py; enforce auth via Unity Catalog connections.\\n\\nLog tool I/O via MLflow Tracing for auditability and evaluation.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 1 documents\n",
      "\n",
      "üìÑ Document Summary:\n",
      "  1. Source: D:\\Work\\E D C\\poc_p1\\data\\docs\\sample_data\\Project_Book_Databricks_Asset_Bundle.docx\n",
      "     Content: Project Book ‚Äî for GenAI & ML\n",
      "\n",
      "Production-ready patterns for ML and LLM/GenAI on Azure Databricks\n",
      "\n",
      "V...\n",
      "     Length: 6753 characters\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load documents\n",
    "docs_dir = config_manager.data.docs_dir\n",
    "print(f\"üìÅ Loading documents from: {docs_dir}\")\n",
    "\n",
    "try:\n",
    "    documents = load_documents(docs_dir)\n",
    "    print(f\"‚úÖ Loaded {len(documents)} documents\")\n",
    "    \n",
    "    # Display document information\n",
    "    if documents:\n",
    "        print(\"\\nüìÑ Document Summary:\")\n",
    "        for i, doc in enumerate(documents[:5]):  # Show first 5\n",
    "            source = doc.metadata.get('source', 'Unknown')\n",
    "            content_preview = doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content\n",
    "            print(f\"  {i+1}. Source: {source}\")\n",
    "            print(f\"     Content: {content_preview}\")\n",
    "            print(f\"     Length: {len(doc.page_content)} characters\")\n",
    "            print()\n",
    "        \n",
    "        if len(documents) > 5:\n",
    "            print(f\"  ... and {len(documents) - 5} more documents\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No documents found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading documents: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "737dba48-6d68-4094-8378-0b8e2de0c8eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Local FAISS Index Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3dabee8-cecc-4202-a8a9-c665b960c225",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è Creating FAISS index for local development...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'D:\\\\Work\\\\E D C\\\\poc_p1\\\\data\\\\docs\\\\sample_data\\\\Project_Book_Databricks_Asset_Bundle.docx'}, page_content='Project Book ‚Äî for GenAI & ML\\n\\nProduction-ready patterns for ML and LLM/GenAI on Azure Databricks\\n\\nVersion: 1.1 | Date: 2025-09-10\\n\\n1. Executive Summary\\n\\nThis Project Book defines a production-grade template and runbook for delivering machine learning (ML) and generative AI (GenAI) applications using Databricks Asset Bundles (DAB). It separates concerns between ML (data science, modeling, evaluation) and MLOps (platform, deployment, governance, observability), and codifies the repository structure, bundle configuration (databricks.yml), CI/CD, security, and operations.\\n\\n2. Scope & Audience\\n\\nAudience: ML Engineers, Data Scientists, Platform/MLOps Engineers, Security & Governance, Product Owners.\\n\\nScope: Applies to both traditional ML (forecasting, classification, etc.) and GenAI (RAG, agents, copilots).\\n\\nCloud/Platform: Azure Databricks with Unity Catalog, MLflow, Mosaic AI, Entra ID, Serverless Compute.\\n\\n3. Architecture Overview\\n\\n3.1 Four-layer Reference (from Project Charter)\\n\\nData Foundation (GraphRAG): Delta Lake + Knowledge Graph (Spark GraphX/GraphFrames) for long-term memory.\\n\\nTooling & Integration (Model Context Protocol, MCP): standardized, secure tool connectors.\\n\\nIntelligence & Reasoning (Multi-Agent System): OpenAI Assistants/Agents orchestration with LangChain.\\n\\nFoundational LLM (Hosted in North America): choice of Llama, GPT, Claude; governed access via Mosaic AI.\\n\\n3.2 Confirmed Stack (from Project Charter)\\n\\nCloud: Microsoft Azure; Platform: Databricks (Serverless where possible).\\n\\nData & AI Platform: Mosaic AI; Governance: Unity Catalog; Tracking: MLflow.\\n\\nAuthN/Z: Microsoft Entra ID (SSO, OAuth/M2M).\\n\\nAgent Framework: OpenAI Assistants, LangChain; UI: Gradio (or app frameworks).\\n\\n\\n\\n4. ML vs MLOps ‚Äî Separation of Concerns\\n\\n4.1 ML (Product/Use-case scope)\\n\\nDefine business problem & KPIs; curate datasets; feature engineering; model training & evaluation.\\n\\nPrompt/agent design, RAG pipelines, tool selection (MCP), offline & online evaluation.\\n\\nRegister models & app artifacts in MLflow / Unity Catalog; author notebooks & src modules; tests.\\n\\n4.2 MLOps (Platform/Operations scope)\\n\\nBundle configuration (databricks.yml), environments (dev/stage/prod) and deployment modes.\\n\\nProvision & manage compute (serverless where possible), permissions, and secret management.\\n\\nObservability (MLflow Tracing), model/app serving, cost/usage tracking via Mosaic AI Gateway.\\n\\nCI/CD automation, policy checks, data/AI governance via Unity Catalog.\\n\\n5. Databricks Asset Bundles (DAB) Overview\\n\\n5.1 Why Bundles\\n\\nDeclarative, versioned deployments for jobs, pipelines, models, serving endpoints, and related resources.\\n\\nEnvironment-specific targets (dev/stage/prod) with validation and deployment modes.\\n\\nRepeatable automation in CI/CD with bundle validate/deploy/run commands.\\n\\n5.2 Key Files & Concepts\\n\\ndatabricks.yml at repo root defines bundle name, targets, artifacts, variables, workspace settings.\\n\\nresources section defines jobs, pipelines, models, connections, volumes, dashboards, apps.\\n\\ntargets allow environment-specific overrides (compute, permissions, schedules, variables).\\n\\n6. Repository & Project Folder Structure\\n\\nUse the following structure for any ML or GenAI project. The /bundle folder contains DAB configuration; ML code is modularized under /src; notebooks are for exploration; pipelines and tests are first-class citizens.\\n\\n/ (repo root)\\n/ ‚îú‚îÄ‚îÄ app/ ‚îÇ   ‚îú‚îÄ‚îÄ app.py ‚îÇ   ‚îú‚îÄ‚îÄ settings.py ‚îÇ   ‚îî‚îÄ‚îÄ prompts/ ‚îÇ       ‚îú‚îÄ‚îÄ system/ ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ agent_system.md ‚îÇ       ‚îî‚îÄ‚îÄ rag/ ‚îÇ           ‚îî‚îÄ‚îÄ retrieval_prompt.md ‚îú‚îÄ‚îÄ src/ ‚îÇ   ‚îú‚îÄ‚îÄ config/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ settings.py ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ secrets_provider.py ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ registry.py ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py ‚îÇ   ‚îú‚îÄ‚îÄ logging/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logger.py ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mlflow_logging.py ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tracing.py ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py ‚îÇ   ‚îú‚îÄ‚îÄ data/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ loaders/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ processing/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schemas/ ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ graph/ ‚îÇ   ‚îú‚îÄ‚îÄ evaluation/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluators/ ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ reports/ ‚îÇ   ‚îú‚îÄ‚îÄ ai/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ training/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inference/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embeddings/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ retrieval/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompts/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agents/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ generation/ ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ registry/ ‚îÇ   ‚îú‚îÄ‚îÄ pipelines/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ training_pipeline.py ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validation_pipeline.py ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ testing_pipeline.py ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ inference_pipeline.py ‚îÇ   ‚îú‚îÄ‚îÄ serving/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ endpoints.py ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validators.py ‚îÇ   ‚îî‚îÄ‚îÄ utils/ ‚îÇ       ‚îú‚îÄ‚îÄ io.py ‚îÇ       ‚îú‚îÄ‚îÄ decorators.py ‚îÇ       ‚îî‚îÄ‚îÄ constants.py ‚îú‚îÄ‚îÄ bundle/ ‚îÇ   ‚îú‚îÄ‚îÄ databricks.yml ‚îÇ   ‚îî‚îÄ‚îÄ resources/ ‚îÇ       ‚îú‚îÄ‚îÄ jobs.yml ‚îÇ       ‚îú‚îÄ‚îÄ pipelines.yml ‚îÇ       ‚îú‚îÄ‚îÄ connections.yml ‚îÇ       ‚îú‚îÄ‚îÄ serving_endpoints.yml ‚îÇ       ‚îî‚îÄ‚îÄ permissions.yml ‚îú‚îÄ‚îÄ docs/ ‚îÇ   ‚îú‚îÄ‚îÄ index.md ‚îÇ   ‚îî‚îÄ‚îÄ architecture.md ‚îú‚îÄ‚îÄ .github/ ‚îÇ   ‚îî‚îÄ‚îÄ workflows/ ‚îÇ       ‚îî‚îÄ‚îÄ databricks_deploy.yml ‚îú‚îÄ‚îÄ requirements.txt ‚îú‚îÄ‚îÄ README.md ‚îî‚îÄ‚îÄ LICENSE\\n\\n7. Governance & Security\\n\\nUnity Catalog: single source of truth for data, models, volumes; assign privileges to groups; avoid user-level grants.\\n\\nUse service principals for automation; manage identities in Entra ID; prefer OAuth tokens over PATs.\\n\\nStore secrets in Unity Catalog connections or secret scopes; never commit secrets.\\n\\nAdopt serverless compute for least-privilege data plane and simpler ops; lock down production endpoints.\\n\\n8. Observability & Monitoring\\n\\nMLflow: experiment tracking, model registry (aliases like @champion/@challenger), lineage to runs.\\n\\nMosaic AI Gateway: usage tracking, payload logging (Delta tables), guardrails, rate limits, fallbacks.\\n\\n\\n\\nMLflow Tracing: capture prompts, tool calls, latencies; wire into dashboards and alerts.\\n\\nQuality monitors: schedule evaluation jobs; track costs/latency & business KPIs.\\n\\n9. Testing & Quality Gates\\n\\nUnit tests for src modules; integration tests for end-to-end pipelines; security tests for auth and inputs.\\n\\nBundle validation in CI; policy checks (lint YAML, prohibited settings).\\n\\nOffline eval datasets; automated acceptance thresholds (retrieval precision/recall, hallucination rate).\\n\\nBlue/green model rollout using model registry aliases; automatic rollback on SLO breaches.\\n\\n10. Runbooks & Operational Procedures\\n\\nCommon Databricks Asset Bundle commands:\\n\\ndatabricks bundle validate -t <target>\\ndatabricks bundle deploy -t <target>\\ndatabricks bundle run <resource-name> -t <target>\\ndatabricks bundle destroy -t <target>\\n\\n11. Appendices\\n\\nA. Sample RAG application structure (adapted)\\n\\nSee notebooks and src layout above; align with your current RAG structure to ensure parity with model tracking and retrieval components.\\n\\nB. MCP Tooling Layer\\n\\nExpose enterprise tools via MCP servers; register tools in agents/tools_mcp.py; enforce auth via Unity Catalog connections.\\n\\nLog tool I/O via MLflow Tracing for auditability and evaluation.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.43it/s]\n",
      "d:\\Work\\E D C\\poc_p1\\src\\rag\\ingest.py:47: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS index created at: None\n",
      "üìä Index contains 9 vectors\n",
      "\n",
      "üîç Test query: 'What is the project charter about?'\n",
      "üìÑ Top 3 results:\n",
      "  1. targets allow environment-specific overrides (compute, permissions, schedules, variables).\n",
      "\n",
      "6. Repository & Project Folder Structure\n",
      "\n",
      "Use the followin...\n",
      "  2. Register models & app artifacts in MLflow / Unity Catalog; author notebooks & src modules; tests.\n",
      "\n",
      "4.2 MLOps (Platform/Operations scope)\n",
      "\n",
      "Bundle confi...\n",
      "  3. 7. Governance & Security\n",
      "\n",
      "Unity Catalog: single source of truth for data, models, volumes; assign privileges to groups; avoid user-level grants.\n",
      "\n",
      "Use ...\n"
     ]
    }
   ],
   "source": [
    "# Create FAISS index for local development\n",
    "if not config_manager.data.use_databricks:\n",
    "    print(\"üèóÔ∏è Creating FAISS index for local development...\")\n",
    "    \n",
    "    try:\n",
    "        index_path = build_index(\n",
    "            docs_dir=config_manager.data.docs_dir,\n",
    "            index_dir=config_manager.data.index_dir,\n",
    "            use_databricks=False\n",
    "        )\n",
    "        print(f\"‚úÖ FAISS index created at: {index_path}\")\n",
    "        \n",
    "        # Test the index\n",
    "        from langchain_community.vectorstores import FAISS\n",
    "        from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "        \n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=config_manager.embedding.model_name\n",
    "        )\n",
    "        \n",
    "        store = FAISS.load_local(\n",
    "            config_manager.data.index_dir, \n",
    "            embeddings, \n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        \n",
    "        print(f\"üìä Index contains {store.index.ntotal} vectors\")\n",
    "        \n",
    "        # Test similarity search\n",
    "        test_query = \"What is the project charter about?\"\n",
    "        results = store.similarity_search(test_query, k=3)\n",
    "        \n",
    "        print(f\"\\nüîç Test query: '{test_query}'\")\n",
    "        print(\"üìÑ Top 3 results:\")\n",
    "        for i, result in enumerate(results):\n",
    "            print(f\"  {i+1}. {result.page_content[:150]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating FAISS index: {e}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping FAISS index creation (Databricks mode enabled)\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "/Workspace/Users/pvenkataswamy@edc.ca/poc_p1/environment.yaml",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_data_preparation",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
