{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Evaluation Notebook\n",
        "\n",
        "This notebook provides comprehensive evaluation tools for testing RAG system performance.\n",
        "\n",
        "## Features\n",
        "- RAG system evaluation metrics\n",
        "- Retrieval accuracy testing\n",
        "- Response quality assessment\n",
        "- A/B testing framework\n",
        "- Performance benchmarking\n",
        "- Response time analysis\n",
        "- Quality scoring\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Add src to path for imports\n",
        "sys.path.append(str(Path.cwd().parent / \"src\"))\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Import our modules\n",
        "from utils import config_manager\n",
        "from rag.chain import create_rag_chain, create_databricks_rag_chain\n",
        "\n",
        "print(\"âœ… Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Framework Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize RAG chain for evaluation\n",
        "print(\"ðŸ”§ Initializing RAG chain for evaluation...\")\n",
        "\n",
        "if config_manager.data.use_databricks:\n",
        "    chain = create_databricks_rag_chain()\n",
        "    print(\"âœ… Databricks RAG chain initialized\")\n",
        "else:\n",
        "    chain = create_rag_chain(\n",
        "        index_dir=config_manager.data.index_dir,\n",
        "        use_databricks=False\n",
        "    )\n",
        "    print(\"âœ… Local RAG chain initialized\")\n",
        "\n",
        "# Define evaluation test cases\n",
        "test_cases = [\n",
        "    {\n",
        "        \"query\": \"What is the project charter about?\",\n",
        "        \"expected_topics\": [\"project\", \"charter\", \"vision\", \"goals\"],\n",
        "        \"category\": \"general\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What technology stack is used?\",\n",
        "        \"expected_topics\": [\"technology\", \"stack\", \"databricks\", \"unity catalog\"],\n",
        "        \"category\": \"technical\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"How do I set up the environment?\",\n",
        "        \"expected_topics\": [\"setup\", \"environment\", \"installation\", \"configuration\"],\n",
        "        \"category\": \"setup\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What are the main features?\",\n",
        "        \"expected_topics\": [\"features\", \"capabilities\", \"functionality\"],\n",
        "        \"category\": \"features\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What is the architecture?\",\n",
        "        \"expected_topics\": [\"architecture\", \"design\", \"components\", \"layers\"],\n",
        "        \"category\": \"architecture\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"ðŸ“‹ Loaded {len(test_cases)} test cases for evaluation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run performance evaluation\n",
        "def evaluate_rag_performance(test_cases: List[Dict], chain) -> pd.DataFrame:\n",
        "    \"\"\"Evaluate RAG performance on test cases.\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    print(\"ðŸ§ª Running performance evaluation...\")\n",
        "    \n",
        "    for i, test_case in enumerate(test_cases):\n",
        "        query = test_case[\"query\"]\n",
        "        expected_topics = test_case[\"expected_topics\"]\n",
        "        category = test_case[\"category\"]\n",
        "        \n",
        "        print(f\"\\\\nTesting {i+1}/{len(test_cases)}: {query}\")\n",
        "        \n",
        "        try:\n",
        "            # Measure response time\n",
        "            start_time = time.time()\n",
        "            response = chain.invoke(query)\n",
        "            end_time = time.time()\n",
        "            \n",
        "            response_time = end_time - start_time\n",
        "            \n",
        "            # Calculate response length\n",
        "            response_length = len(response)\n",
        "            \n",
        "            # Simple topic coverage check\n",
        "            response_lower = response.lower()\n",
        "            topics_found = sum(1 for topic in expected_topics if topic.lower() in response_lower)\n",
        "            topic_coverage = topics_found / len(expected_topics)\n",
        "            \n",
        "            # Quality indicators\n",
        "            has_sources = \"[source:\" in response.lower()\n",
        "            is_comprehensive = response_length > 100\n",
        "            is_concise = response_length < 1000\n",
        "            \n",
        "            results.append({\n",
        "                \"query\": query,\n",
        "                \"category\": category,\n",
        "                \"response_time\": response_time,\n",
        "                \"response_length\": response_length,\n",
        "                \"topic_coverage\": topic_coverage,\n",
        "                \"topics_found\": topics_found,\n",
        "                \"total_topics\": len(expected_topics),\n",
        "                \"has_sources\": has_sources,\n",
        "                \"is_comprehensive\": is_comprehensive,\n",
        "                \"is_concise\": is_concise,\n",
        "                \"response\": response\n",
        "            })\n",
        "            \n",
        "            print(f\"  âœ… Response time: {response_time:.2f}s, Coverage: {topic_coverage:.2f}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  âŒ Error: {e}\")\n",
        "            results.append({\n",
        "                \"query\": query,\n",
        "                \"category\": category,\n",
        "                \"response_time\": None,\n",
        "                \"response_length\": 0,\n",
        "                \"topic_coverage\": 0,\n",
        "                \"topics_found\": 0,\n",
        "                \"total_topics\": len(expected_topics),\n",
        "                \"has_sources\": False,\n",
        "                \"is_comprehensive\": False,\n",
        "                \"is_concise\": False,\n",
        "                \"response\": f\"Error: {e}\"\n",
        "            })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Run evaluation\n",
        "results_df = evaluate_rag_performance(test_cases, chain)\n",
        "\n",
        "print(f\"\\\\nðŸ“Š Evaluation complete! Processed {len(results_df)} test cases.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
