{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Evaluation Notebook\n",
        "\n",
        "This notebook provides comprehensive evaluation tools for testing RAG system performance.\n",
        "\n",
        "## Features\n",
        "- RAG system evaluation metrics\n",
        "- Retrieval accuracy testing\n",
        "- Response quality assessment\n",
        "- A/B testing framework\n",
        "- Performance benchmarking\n",
        "- Response time analysis\n",
        "- Quality scoring\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Setup complete!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "# Add src to path for imports\n",
        "PROJECT_ROOT = Path.cwd().parent\n",
        "sys.path.append(str(PROJECT_ROOT))\n",
        "\n",
        "\n",
        "# Import our modules\n",
        "from src.utils.config import config_manager\n",
        "from src.rag.chain import create_rag_chain, create_databricks_rag_chain\n",
        "\n",
        "print(\"✅ Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Framework Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Initializing RAG chain for evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Work\\E D C\\poc_p1\\src\\rag\\chain.py:42: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Local RAG chain initialized\n",
            "📋 Loaded 5 test cases for evaluation\n"
          ]
        }
      ],
      "source": [
        "# Initialize RAG chain for evaluation\n",
        "print(\"🔧 Initializing RAG chain for evaluation...\")\n",
        "\n",
        "if config_manager.data.use_databricks:\n",
        "    chain = create_databricks_rag_chain()\n",
        "    print(\"✅ Databricks RAG chain initialized\")\n",
        "else:\n",
        "    chain = create_rag_chain(\n",
        "        index_dir=config_manager.data.index_dir,\n",
        "        use_databricks=False\n",
        "    )\n",
        "    print(\"✅ Local RAG chain initialized\")\n",
        "\n",
        "# Define evaluation test cases\n",
        "test_cases = [\n",
        "    {\n",
        "        \"query\": \"What is the project charter about?\",\n",
        "        \"expected_topics\": [\"project\", \"charter\", \"vision\", \"goals\"],\n",
        "        \"category\": \"general\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What technology stack is used?\",\n",
        "        \"expected_topics\": [\"technology\", \"stack\", \"databricks\", \"unity catalog\"],\n",
        "        \"category\": \"technical\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"How do I set up the environment?\",\n",
        "        \"expected_topics\": [\"setup\", \"environment\", \"installation\", \"configuration\"],\n",
        "        \"category\": \"setup\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What are the main features?\",\n",
        "        \"expected_topics\": [\"features\", \"capabilities\", \"functionality\"],\n",
        "        \"category\": \"features\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What is the architecture?\",\n",
        "        \"expected_topics\": [\"architecture\", \"design\", \"components\", \"layers\"],\n",
        "        \"category\": \"architecture\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"📋 Loaded {len(test_cases)} test cases for evaluation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧪 Running performance evaluation...\n",
            "\\nTesting 1/5: What is the project charter about?\n",
            "  ✅ Response time: 49.73s, Coverage: 0.50\n",
            "\\nTesting 2/5: What technology stack is used?\n",
            "  ✅ Response time: 39.83s, Coverage: 1.00\n",
            "\\nTesting 3/5: How do I set up the environment?\n",
            "  ✅ Response time: 57.54s, Coverage: 0.25\n",
            "\\nTesting 4/5: What are the main features?\n",
            "  ❌ Error: Request timed out.\n",
            "\\nTesting 5/5: What is the architecture?\n",
            "  ✅ Response time: 50.70s, Coverage: 0.25\n",
            "\\n📊 Evaluation complete! Processed 5 test cases.\n"
          ]
        }
      ],
      "source": [
        "# Run performance evaluation\n",
        "def evaluate_rag_performance(test_cases: List[Dict], chain) -> pd.DataFrame:\n",
        "    \"\"\"Evaluate RAG performance on test cases.\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    print(\"🧪 Running performance evaluation...\")\n",
        "    \n",
        "    for i, test_case in enumerate(test_cases):\n",
        "        query = test_case[\"query\"]\n",
        "        expected_topics = test_case[\"expected_topics\"]\n",
        "        category = test_case[\"category\"]\n",
        "        \n",
        "        print(f\"\\\\nTesting {i+1}/{len(test_cases)}: {query}\")\n",
        "        \n",
        "        try:\n",
        "            # Measure response time\n",
        "            start_time = time.time()\n",
        "            response = chain.invoke(query)\n",
        "            end_time = time.time()\n",
        "            \n",
        "            response_time = end_time - start_time\n",
        "            \n",
        "            # Calculate response length\n",
        "            response_length = len(response)\n",
        "            \n",
        "            # Simple topic coverage check\n",
        "            response_lower = response.lower()\n",
        "            topics_found = sum(1 for topic in expected_topics if topic.lower() in response_lower)\n",
        "            topic_coverage = topics_found / len(expected_topics)\n",
        "            \n",
        "            # Quality indicators\n",
        "            has_sources = \"[source:\" in response.lower()\n",
        "            is_comprehensive = response_length > 100\n",
        "            is_concise = response_length < 1000\n",
        "            \n",
        "            results.append({\n",
        "                \"query\": query,\n",
        "                \"category\": category,\n",
        "                \"response_time\": response_time,\n",
        "                \"response_length\": response_length,\n",
        "                \"topic_coverage\": topic_coverage,\n",
        "                \"topics_found\": topics_found,\n",
        "                \"total_topics\": len(expected_topics),\n",
        "                \"has_sources\": has_sources,\n",
        "                \"is_comprehensive\": is_comprehensive,\n",
        "                \"is_concise\": is_concise,\n",
        "                \"response\": response\n",
        "            })\n",
        "            \n",
        "            print(f\"  ✅ Response time: {response_time:.2f}s, Coverage: {topic_coverage:.2f}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Error: {e}\")\n",
        "            results.append({\n",
        "                \"query\": query,\n",
        "                \"category\": category,\n",
        "                \"response_time\": None,\n",
        "                \"response_length\": 0,\n",
        "                \"topic_coverage\": 0,\n",
        "                \"topics_found\": 0,\n",
        "                \"total_topics\": len(expected_topics),\n",
        "                \"has_sources\": False,\n",
        "                \"is_comprehensive\": False,\n",
        "                \"is_concise\": False,\n",
        "                \"response\": f\"Error: {e}\"\n",
        "            })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Run evaluation\n",
        "results_df = evaluate_rag_performance(test_cases, chain)\n",
        "\n",
        "print(f\"\\\\n📊 Evaluation complete! Processed {len(results_df)} test cases.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
